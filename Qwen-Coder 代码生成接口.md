# äºŒã€ Qwen-Coder ä»£ç ç”Ÿæˆæ¥å£

## 1. æ¦‚è§ˆ

- **API åç§°**: Qwen-Coder ä»£ç ç”Ÿæˆæ¥å£ (Qwen-Coder-30B)
- **æ¥å£çŠ¶æ€**: `[ç¨³å®š]`
- **æ¥å£æè¿°**: æä¾›åŸºäº Qwen-Coder-30B å¤§æ¨¡å‹çš„ä»£ç ç”Ÿæˆä¸å¯¹è¯èƒ½åŠ›ã€‚å®ƒæ”¯æŒä¸¤ç§ç‹¬ç«‹çš„è°ƒç”¨æ¨¡å¼ï¼š
  1. **å•è½®ä»£ç ç”Ÿæˆ**: ç”¨äºä¸€æ¬¡æ€§çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œç›´æ¥è¿”å›å®Œæ•´ç»“æœã€‚
  2. **æµå¼å¤šè½®å¯¹è¯**: ç”¨äºéœ€è¦ä¸Šä¸‹æ–‡è®°å¿†çš„è¿ç»­å¯¹è¯ï¼Œå®æ—¶è¿”å›ç”Ÿæˆå†…å®¹ã€‚

## 2. æ¥å£ 1: å•è½®ä»£ç ç”Ÿæˆ

æ­¤æ¥å£é€‚ç”¨äºç‹¬ç«‹çš„ã€æ— ä¸Šä¸‹æ–‡å…³è”çš„ä»£ç ç”Ÿæˆè¯·æ±‚ã€‚

### 2.1. è¯·æ±‚

- **Endpoint (ç«¯ç‚¹)**
  - `POST http://10.3.35.21:8001/v1/code/generate`
- **è¯·æ±‚å‚æ•°**
  - è¯·æ±‚ä½“ä¸º `application/x-www-form-urlencoded`ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| å‚æ•°å   | ç±»å‹   | æ˜¯å¦å¿…å¡« | æè¿°                                                |
| -------- | ------ | -------- | --------------------------------------------------- |
| `prompt` | string | æ˜¯       | å…·ä½“çš„ä»£ç ç”ŸæˆæŒ‡ä»¤ï¼Œä¾‹å¦‚ "ç”¨Pythonå†™ä¸€ä¸ªå†’æ³¡æ’åº"ã€‚ |

### 2.2. å“åº” 

- **æˆåŠŸå“åº”**

  - **HTTP çŠ¶æ€ç **: `200 OK`

  - **å“åº”ä½“ (JSON ç¤ºä¾‹)**:

    ```shell
    {
      "status": "success",
      "result": "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]"
    }
    ```

- **å¤±è´¥å“åº”**

  - **HTTP çŠ¶æ€ç **: `500 Internal Server Error`

  - **å“åº”ä½“ (JSON ç¤ºä¾‹)**:

    ```
    {
      "detail": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: [å…·ä½“çš„é”™è¯¯ä¿¡æ¯]"
    }
    ```

### 2.3. Python è°ƒç”¨ç¤ºä¾‹

```python
import requests

SERVER_URL = "[http://10.3.35.21:8001](http://10.3.35.21:8001)"
API_ENDPOINT = f"{SERVER_URL}/v1/code/generate"

PROMPT = "ç”¨ Python å†™ä¸€ä¸ªæœ€åŸºç¡€ç‰ˆçš„å†’æ³¡æ’åºç®—æ³•ï¼Œåªå†™å‡½æ•°ä½“å³å¯ï¼Œä¸å¿…å†™æµ‹è¯•ä»£ç å’Œç¤ºä¾‹"

def call_single_generation():
    """è°ƒç”¨å•è½®ä»£ç ç”Ÿæˆæ¥å£"""
    print("="*60)
    print(f"ğŸš€ æ­£åœ¨å‘å•è½®æ¥å£å‘é€è¯·æ±‚...")
    print(f"ğŸ“ æŒ‡ä»¤: {PROMPT}")
    print("="*60)

    try:
        # æ„é€ è¯·æ±‚æ•°æ®
        data = {'prompt': PROMPT}
        # Coderæ¨¡å‹æ¨ç†è¾ƒæ…¢ï¼Œè®¾ç½®é•¿è¶…æ—¶
        response = requests.post(API_ENDPOINT, data=data, timeout=360)

        if response.status_code == 200:
            result = response.json()
            print("âœ… è¯·æ±‚æˆåŠŸï¼æ¨¡å‹è¿”å›çš„å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š")
            print("-" * 60)
            print(result.get('result'))
            print("-" * 60)
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼")
            print(f"  - çŠ¶æ€ç : {response.status_code}")
            print(f"  - é”™è¯¯ä¿¡æ¯: {response.text}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸ï¼è¯·æ£€æŸ¥æœåŠ¡å™¨åœ°å€æˆ–ç½‘ç»œè¿æ¥ã€‚")
        print(f"  - é”™è¯¯è¯¦æƒ…: {e}")

if __name__ == "__main__":
    call_single_generation()
```

## 3. æ¥å£ 2: æµå¼å¤šè½®ä»£ç å¯¹è¯

æ­¤æ¥å£é€‚ç”¨äºéœ€è¦è¿ç»­å¯¹è¯ã€ä¿æŒä¸Šä¸‹æ–‡è®°å¿†çš„åœºæ™¯ï¼Œå¹¶ä»¥æµå¼æ–¹å¼å®æ—¶è¿”å›ç»“æœã€‚

### 3.1. è¯·æ±‚

- **Endpoint (ç«¯ç‚¹)**
  - `POST http://10.3.35.21:8001/v1/code/chat_stream`
- **è¯·æ±‚å‚æ•° **
  - è¯·æ±‚ä½“ä¸º `application/x-www-form-urlencoded`ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| å‚æ•°å            | ç±»å‹   | æ˜¯å¦å¿…å¡« | æè¿°                                                         |
| ----------------- | ------ | -------- | ------------------------------------------------------------ |
| `prompt`          | string | æ˜¯       | å½“å‰è½®æ¬¡çš„å¯¹è¯å†…å®¹ã€‚                                         |
| `conversation_id` | string | å¦       | ä¼šè¯IDã€‚**é¦–æ¬¡è¯·æ±‚æ—¶æ— éœ€æä¾›**ï¼ŒæœåŠ¡å™¨ä¼šè‡ªåŠ¨åˆ›å»ºå¹¶è¿”å›ã€‚**åç»­è¯·æ±‚å¿…é¡»æºå¸¦æ­¤ID**ä»¥ç»´æŒä¸Šä¸‹æ–‡ã€‚ |

### 3.2. å“åº”

- **HTTP çŠ¶æ€ç **: `200 OK`
- **å“åº”æ ¼å¼**: `text/event-stream`
  - **é¦–æ¬¡è¯·æ±‚**: å“åº”æµçš„å¼€å¤´ä¼šåŒ…å«ä¼šè¯IDï¼Œæ ¼å¼ä¸º `conversation_id:[UUID]\n---\n`ï¼Œç´§æ¥ç€æ˜¯æ¨¡å‹ç”Ÿæˆçš„å†…å†…å®¹ã€‚å®¢æˆ·ç«¯éœ€è¦è§£æå¹¶ä¿å­˜æ­¤IDç”¨äºåç»­å¯¹è¯ã€‚
  - **åç»­è¯·æ±‚**: å“åº”æµåªåŒ…å«æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ã€‚

### 3.3. Python è°ƒç”¨ç¤ºä¾‹ 

```python
import requests
import time

SERVER_URL = "[http://10.3.35.21:8001](http://10.3.35.21:8001)"
API_ENDPOINT = f"{SERVER_URL}/v1/code/chat_stream"

def call_streaming_chat_session():
    """æ¼”ç¤ºä¸€ä¸ªå®Œæ•´çš„æµå¼å¤šè½®å¯¹è¯"""
    conversation_id = None

    # --- ç¬¬ä¸€è½®å¯¹è¯ï¼šç”ŸæˆPythonä»£ç  ---
    prompt1 = "ç”¨ Python å†™ä¸€ä¸ªæœ€åŸºç¡€ç‰ˆçš„å†’æ³¡æ’åºç®—æ³•ï¼Œåªå†™å‡½æ•°ä½“å³å¯ï¼Œä¸å¿…å†™æµ‹è¯•ä»£ç å’Œç¤ºä¾‹"
    print("="*60)
    print(f"ğŸš€ [ç¬¬ä¸€è½®] æ­£åœ¨å‘æµå¼æ¥å£å‘é€è¯·æ±‚...")
    print(f"ğŸ“ ä½ : {prompt1}")
    print("="*60)
    print("ğŸ¤– AI: ", end="", flush=True)

    try:
        data = {'prompt': prompt1}
        with requests.post(API_ENDPOINT, data=data, stream=True, timeout=360) as response:
            if response.status_code == 200:
                header_processed = False
                for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                    if not header_processed and "conversation_id:" in chunk:
                        parts = chunk.split("\n---\n", 1)
                        header = parts[0]
                        conversation_id = header.replace("conversation_id:", "").strip()
                        print(f"\n[ç³»ç»Ÿæç¤ºï¼šå·²å»ºç«‹ä¼šè¯, ID: {conversation_id}]\n")
                        print("ğŸ¤– AI: ", end="", flush=True)
                        if len(parts) > 1: print(parts[1], end="", flush=True)
                        header_processed = True
                        continue
                    print(chunk, end="", flush=True)
                print("\n") # å®Œæ•´å›ç­”ç»“æŸåæ¢è¡Œ
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼çŠ¶æ€ç : {response.status_code}, é”™è¯¯: {response.text}")
                return # å¦‚æœç¬¬ä¸€è½®å¤±è´¥ï¼Œåˆ™ç»ˆæ­¢
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
        return

    time.sleep(1)

    # --- ç¬¬äºŒè½®å¯¹è¯ï¼šå°†Pythonä»£ç è½¬ä¸ºJava ---
    if not conversation_id:
        print("âŒ æœªèƒ½è·å–ä¼šè¯IDï¼Œæ— æ³•è¿›è¡Œç¬¬äºŒè½®å¯¹è¯ã€‚")
        return

    prompt2 = "éå¸¸å¥½ã€‚ç°åœ¨ï¼Œè¯·æŠŠä¸Šé¢è¿™æ®µ Python ä»£ç æ— ç¼åœ°è½¬æ¢æˆåŠŸèƒ½å®Œå…¨ç­‰æ•ˆçš„ Java ä»£ç ã€‚"
    print("="*60)
    print(f"ğŸš€ [ç¬¬äºŒè½®] æ­£åœ¨ç»§ç»­å¯¹è¯...")
    print(f"ğŸ“ ä½ : {prompt2}")
    print("="*60)
    print("ğŸ¤– AI: ", end="", flush=True)

    try:
        data = {'prompt': prompt2, 'conversation_id': conversation_id}
        with requests.post(API_ENDPOINT, data=data, stream=True, timeout=360) as response:
            if response.status_code == 200:
                for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                    # ç¬¬äºŒè½®ä¸å†éœ€è¦å¤„ç†IDï¼Œç›´æ¥æ‰“å°
                    if "conversation_id:" in chunk and "---" in chunk: continue
                    print(chunk, end="", flush=True)
                print("\n")
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼çŠ¶æ€ç : {response.status_code}, é”™è¯¯: {response.text}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")

if __name__ == "__main__":
    call_streaming_chat_session()
```

## 4. æ³¨æ„äº‹é¡¹

1. **è¶…æ—¶æ—¶é—´**ï¼šCoder-30B æ¨¡å‹è¾ƒå¤§ï¼Œæ¨ç†è€—æ—¶è¾ƒé•¿ã€‚å®¢æˆ·ç«¯è°ƒç”¨æ—¶ï¼Œ`timeout` å‚æ•°å»ºè®®è®¾ç½®ä¸º **360ç§’** æˆ–æ›´é«˜ã€‚
2. **ä¼šè¯ç®¡ç†**ï¼šæµå¼å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¿å­˜åœ¨æœåŠ¡å™¨å†…å­˜ä¸­ã€‚å¦‚æœæœåŠ¡é‡å¯ï¼Œæ‰€æœ‰ä¼šè¯å†å²å°†ä¸¢å¤±ã€‚

##  è¿ç»´ç¬”è®°

- **æ¥å£ç«¯å£**: `8001`
- **é…ç½®é¡¹åœ°å€**: `/etc/systemd/system/qwen_coder_api.service`
- **ä»£ç ä½ç½®**: `/data/wangmeng/qwen_llm_api/qwen_coder/qwen_coder_api.py`
- **æ¥å£æºä»£ç **:

```python
# -------------------------------------------------------------------------
# Qwen3-Coder-30B API æœåŠ¡
# åŠŸèƒ½: æä¾›å•è½®ä»£ç ç”Ÿæˆå’Œæµå¼å¤šè½®ä»£ç å¯¹è¯èƒ½åŠ›
# åŠ è½½æ–¹å¼: ä½¿ç”¨åŸç”Ÿ Transformers ä»æœ¬åœ°è·¯å¾„åŠ è½½
# -------------------------------------------------------------------------

import torch
import uvicorn
from fastapi import FastAPI, Form, HTTPException
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
import uuid
import logging
from typing import List, Dict, Optional
from threading import Thread
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

# ======================== å…¨å±€é…ç½® ========================
# 1. æ¨¡å‹æœ¬åœ°ç»å¯¹è·¯å¾„
MODEL_PATH = "/data/LLM/Qwen/qwen3-coder-30b-a3b-instruct-local/"

# 2. API æœåŠ¡ç›‘å¬ç«¯å£
API_PORT = 8001

# 3. æ¨¡å‹ç”Ÿæˆå‚æ•°
GENERATION_CONFIG = {
    "max_new_tokens": 4096,
    "temperature": 0.7,
    "top_p": 0.8,
    "top_k": 20,
    "repetition_penalty": 1.05
}

# 4. æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# ==========================================================

# --- å…¨å±€å˜é‡å®šä¹‰ ---
model = None
tokenizer = None
conversation_histories: Dict[str, List] = {}

# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global model, tokenizer
    logging.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {MODEL_PATH} ...")
    logging.info("è¿™å°†å ç”¨çº¦ 75GB æ˜¾å­˜ï¼Œè¯·ç¡®ä¿ç¡¬ä»¶èµ„æºå……è¶³ã€‚")

    # ä½¿ç”¨ transformers çš„åŠ è½½å™¨ï¼Œå®ƒèƒ½æ­£ç¡®å¤„ç†æœ¬åœ°è·¯å¾„
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        local_files_only=True,
        trust_remote_code=True # å¯¹äºQwenç­‰å¤æ‚æ¨¡å‹ï¼Œå¿…é¡»ä¿¡ä»»å…¶è‡ªå¸¦çš„ä»£ç 
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        dtype="auto",
        device_map="auto",
        local_files_only=True,
        trust_remote_code=True # å¿…é¡»
    )

    logging.info("æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒCoder APIæœåŠ¡å‡†å¤‡å°±ç»ªï¼")
    yield

    model = None
    tokenizer = None
    torch.cuda.empty_cache()
    logging.info("æ¨¡å‹èµ„æºå·²é‡Šæ”¾ï¼ŒæœåŠ¡å·²å…³é—­ã€‚")

# --- åˆ›å»º FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(lifespan=lifespan)

# ======================== API æ¥å£å®šä¹‰ ========================

# --- æ¥å£ 1: å•è½®ä»£ç ç”Ÿæˆ (Stateless) ---
@app.post("/v1/code/generate", summary="å•è½®ä»£ç ç”Ÿæˆ")
async def generate_code(prompt: str = Form(..., description="å•æ¬¡çš„ä»£ç ç”ŸæˆæŒ‡ä»¤")):
    logging.info(f"[å•è½®è¯·æ±‚] Prompt: {prompt[:100]}...")
    try:
        messages = [{"role": "user", "content": prompt}]
        
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        generated_ids = model.generate(**model_inputs, **GENERATION_CONFIG)
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        result = tokenizer.decode(output_ids, skip_special_tokens=True)
        
        logging.info(f"[å•è½®å“åº”] Result: {result[:100]}...")
        return {"status": "success", "result": result}
    except Exception as e:
        logging.error(f"[å•è½®è¯·æ±‚] å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# --- æ¥å£ 2: æµå¼å¤šè½®ä»£ç å¯¹è¯ (Stateful) ---
@app.post("/v1/code/chat_stream", summary="[æµå¼] å¤šè½®ä»£ç å¯¹è¯")
async def stream_chat_with_coder(
    prompt: str = Form(..., description="å½“å‰è½®æ¬¡çš„å¯¹è¯å†…å®¹"),
    conversation_id: Optional[str] = Form(None, description="ä¼šè¯IDï¼Œç”¨äºç»´æŒä¸Šä¸‹æ–‡")
):
    logging.info(f"[æµå¼è¯·æ±‚] Conv ID: {conversation_id}, Prompt: {prompt[:100]}...")

    if not conversation_id:
        conversation_id = str(uuid.uuid4())
        conversation_histories[conversation_id] = []
        logging.info(f"[æµå¼è¯·æ±‚] åˆ›å»ºæ–°ä¼šè¯: {conversation_id}")

    history = conversation_histories.get(conversation_id, [])
    history.append({"role": "user", "content": prompt})

    text = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    generation_kwargs = dict(**model_inputs, streamer=streamer, **GENERATION_CONFIG)
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    async def stream_generator():
        full_response = ""
        # åœ¨æµçš„å¼€å¤´ï¼Œå…ˆå‘é€ conversation_id å’Œä¸€ä¸ªåˆ†éš”ç¬¦
        yield f"conversation_id:{conversation_id}\n---\n"
        
        for new_text in streamer:
            full_response += new_text
            yield new_text
        
        # ç”Ÿæˆç»“æŸåï¼Œå°†å®Œæ•´çš„å›ç­”å­˜å…¥å†å²è®°å½•
        history.append({"role": "assistant", "content": full_response})
        conversation_histories[conversation_id] = history
        logging.info(f"[æµå¼å“åº”] Conv ID: {conversation_id}, Full Response: {full_response[:100]}...")

    return StreamingResponse(stream_generator(), media_type="text/event-stream")

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    logging.info(f"å¯åŠ¨æœåŠ¡ï¼Œç›‘å¬ [http://0.0.0.0](http://0.0.0.0):{API_PORT}")
    uvicorn.run(app, host="0.0.0.0", port=API_PORT)
```

